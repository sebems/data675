{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# From Linear Regression in NumPy to Logistic Regression in PyTorch\n\n## Goals\n\n- Compare implementing regression in NumPy vs PyTorch\n- Extend regression to classification by adding softmax and cross-entropy\n- Practice working with PyTorch's basic APIs","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"markdown","source":"Let's import necessary modules: *pandas* and NumPy for data wrangling, Matplotlib for plotting, and some sklearn utilities. Now we'll also be importing PyTorch.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nimport scipy.optimize\n\n# This utility function (inspired by https://github.com/HIPS/autograd/blob/master/autograd/misc/flatten.py)\n# is useful for optimizing using scipy.optimize.minimize.\ndef flatten(arrs):\n    \"\"\"Return a 1D array containing the elements of the input arrays,\n    and an unflatten function that takes a flattened array and returns\n    the original arrays.\n    \"\"\"\n    arrs = [np.asarray(arr) for arr in arrs]\n    shapes = [arr.shape for arr in arrs]\n    flat = np.concatenate([arr.ravel() for arr in arrs])\n    start_indices = np.cumsum([0] + [arr.size for arr in arrs])\n    def unflatten(params):\n        return [params[start_indices[i]:start_indices[i+1]].reshape(shape)\n                for i, shape in enumerate(shapes)]\n    return flat, unflatten\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T23:36:12.323827Z","iopub.execute_input":"2025-02-27T23:36:12.324060Z","iopub.status.idle":"2025-02-27T23:36:17.100000Z","shell.execute_reply.started":"2025-02-27T23:36:12.324039Z","shell.execute_reply":"2025-02-27T23:36:17.099338Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"We'll load the data as before.","metadata":{}},{"cell_type":"code","source":"ames = pd.read_csv('https://github.com/kcarnold/AmesHousing/blob/master/data/ames.csv.gz?raw=true', compression=\"gzip\")\names['price'] = ames[\"Sale_Price\"] / 100_000 # Make `price` be in units of $100k, to be easier to interpret.\n\n# Create price categories (0, 1, or 2) for the classification task\nn_classes = 3\names['price_rank'] = ames['price'].rank(pct=True)\names['price_bin'] = pd.cut(ames['price_rank'], bins=n_classes, labels=False)\n\n# Prepare features and target\nfeature_names = ['Longitude', 'Latitude']\nX = ames[feature_names].values\ny = ames['price_bin'].values\n\n# standardize the features, to make the optimization more stable\nX_mean = X.mean(axis=0)\nX_std = X.std(axis=0)\nX = (X - X_mean) / X_std\n\n# Split data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T23:37:00.771987Z","iopub.execute_input":"2025-02-27T23:37:00.772339Z","iopub.status.idle":"2025-02-27T23:37:01.513185Z","shell.execute_reply.started":"2025-02-27T23:37:00.772312Z","shell.execute_reply":"2025-02-27T23:37:01.512294Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Basic EDA","metadata":{}},{"cell_type":"markdown","source":"Histogram of target values:","metadata":{}},{"cell_type":"code","source":"plt.hist(y_train);\names['price_bin'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T23:37:44.435078Z","iopub.execute_input":"2025-02-27T23:37:44.435406Z","iopub.status.idle":"2025-02-27T23:37:44.690422Z","shell.execute_reply.started":"2025-02-27T23:37:44.435379Z","shell.execute_reply":"2025-02-27T23:37:44.689578Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"price_bin\n0    981\n1    980\n2    969\nName: count, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"Check shapes","metadata":{}},{"cell_type":"code","source":"print(f\"train: X.shape={X_train.shape}, y.shape={y_train.shape}\")\nprint(f\"valid: X.shape={X_valid.shape}, y.shape={y_valid.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T23:38:28.068742Z","iopub.execute_input":"2025-02-27T23:38:28.069073Z","iopub.status.idle":"2025-02-27T23:38:28.073939Z","shell.execute_reply.started":"2025-02-27T23:38:28.069049Z","shell.execute_reply":"2025-02-27T23:38:28.073250Z"}},"outputs":[{"name":"stdout","text":"train: X.shape=(2344, 2), y.shape=(2344,)\nvalid: X.shape=(586, 2), y.shape=(586,)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Histogram of input feature values","metadata":{}},{"cell_type":"code","source":"plt.hist(X_train.flatten(), bins=30);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T23:38:30.818297Z","iopub.execute_input":"2025-02-27T23:38:30.818613Z","iopub.status.idle":"2025-02-27T23:38:31.010404Z","shell.execute_reply.started":"2025-02-27T23:38:30.818589Z","shell.execute_reply":"2025-02-27T23:38:31.009741Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Part 1: Classification the wrong way (using linear regression)\n\nLet's start by treating this as a regression problem - predicting the class number (0, 1, or 2) directly. We'll fit a linear regression model to the target as if it were a continuous variable. **This isn't the right way to do it**, but it will help us build up to the correct way.","metadata":{}},{"cell_type":"markdown","source":"### 1.A Using NumPy\n\nYou've seen the code below already. Fill in the blanks to complete the linear regression model in NumPy.","metadata":{}},{"cell_type":"code","source":"# Initialize parameters\nnp.random.seed(42)\nn_samples, n_features = X_train.shape\n\ninitial_weights = np.random.standard_normal(size=(2,)) # 2 for each feature (Lon, Lat)\ninitial_bias = np.random.standard_normal()\n\ndef linear_forward(X, weights, bias):\n    return X @ weights + bias\n\ndef compute_mse_loss(y_true, y_pred):\n    return ((y_true - y_pred) ** 2).mean()\n\ndef loss_given_params(params, X, y_true):\n    weights, bias = unpack_params(params)\n    y_pred = linear_forward(X, weights, bias)\n    return compute_mse_loss(y_true, y_pred)\n\ninitial_params, unpack_params = flatten([initial_weights, initial_bias])\ninitial_loss = loss_given_params(initial_params, X_train, y_train)\noptimization_result = scipy.optimize.minimize(loss_given_params, initial_params,\n                                              args=(X_train, y_train))\nfitted_weights_np, fitted_bias_np = unpack_params(optimization_result.x)\nprint(f\"Initial loss: {initial_loss:.2f}\")\nprint(f\"Final loss: {optimization_result.fun:.2f}\")\nprint(f\"Fitted weights: {fitted_weights_np}\")\nprint(f\"Fitted bias: {fitted_bias_np}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T00:28:32.467139Z","iopub.execute_input":"2025-02-28T00:28:32.467513Z","iopub.status.idle":"2025-02-28T00:28:32.482038Z","shell.execute_reply.started":"2025-02-28T00:28:32.467486Z","shell.execute_reply":"2025-02-28T00:28:32.481116Z"}},"outputs":[{"name":"stdout","text":"Initial loss: 1.33\nFinal loss: 0.56\nFitted weights: [-0.26364467  0.17475394]\nFitted bias: 0.9886359240307222\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"Let's evaluate the \"accuracy\" of this model. We'll round the predictions to the nearest integer to get the predicted class.","metadata":{}},{"cell_type":"code","source":"y_pred_valid = linear_forward(X_valid, fitted_weights_np, fitted_bias_np).round()\ny_pred_valid[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T23:45:54.615026Z","iopub.execute_input":"2025-02-27T23:45:54.615405Z","iopub.status.idle":"2025-02-27T23:45:54.621806Z","shell.execute_reply.started":"2025-02-27T23:45:54.615379Z","shell.execute_reply":"2025-02-27T23:45:54.621014Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"accuracy = (y_pred_valid == y_valid).mean()\nprint(f\"Validation accuracy for linear regression 'classifier': {accuracy:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T23:46:17.944477Z","iopub.execute_input":"2025-02-27T23:46:17.944764Z","iopub.status.idle":"2025-02-27T23:46:17.949466Z","shell.execute_reply.started":"2025-02-27T23:46:17.944743Z","shell.execute_reply":"2025-02-27T23:46:17.948462Z"}},"outputs":[{"name":"stdout","text":"Validation accuracy for linear regression 'classifier': 0.31\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### 1.B: Using PyTorch\n\nNow we'll implement the same (wrong) model in PyTorch.\n\nWe'll build this together step by step. Notice that we're still doing this \"the wrong way\", as regression, not classification.","metadata":{}},{"cell_type":"code","source":"# Convert data to PyTorch tensors\nX_train_torch = torch.tensor(X_train, dtype=torch.float32)\nX_valid_torch = torch.tensor(X_valid, dtype=torch.float32)\n# Note: still treating as regression\ny_train_torch = torch.tensor(y_train, dtype=torch.float32)\ny_valid_torch = torch.tensor(y_valid, dtype=torch.float32)\n\nclass SimpleLinear(nn.Module):\n    def __init__(self, n_features):\n        super().__init__()\n        self.weights = nn.Parameter(torch.randn(n_features))\n        self.bias = nn.Parameter(torch.randn(1))\n    \n    def forward(self, x):\n        return x @ self.weights + self.bias\n\n# Training loop\nmodel = SimpleLinear(n_features)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\nn_epochs = 100  # epoch--number of times through the training set\nfor epoch in range(n_epochs):\n    # Forward pass\n    y_pred = model(X_train_torch)\n    loss = F.mse_loss(y_pred, y_train_torch)\n    \n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if epoch % 10 == 0:\n        print(f'Epoch {epoch:02d}, Loss: {loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T00:38:27.756394Z","iopub.execute_input":"2025-02-28T00:38:27.756685Z","iopub.status.idle":"2025-02-28T00:38:27.814949Z","shell.execute_reply.started":"2025-02-28T00:38:27.756664Z","shell.execute_reply":"2025-02-28T00:38:27.814282Z"}},"outputs":[{"name":"stdout","text":"Epoch 00, Loss: 3.0010\nEpoch 10, Loss: 0.5888\nEpoch 20, Loss: 0.5575\nEpoch 30, Loss: 0.5571\nEpoch 40, Loss: 0.5571\nEpoch 50, Loss: 0.5571\nEpoch 60, Loss: 0.5571\nEpoch 70, Loss: 0.5571\nEpoch 80, Loss: 0.5571\nEpoch 90, Loss: 0.5571\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Part 2: Converting to Classification\n\nNow we're going to switch to doing it the right way, as a classification problem. We'll use the softmax function to convert the model's outputs into probabilities, and the cross-entropy loss to train the model to be least surprised by the true class.\n\nAgain we'll start off by implementing this in NumPy, and then extend it to PyTorch.","metadata":{}},{"cell_type":"markdown","source":"### 2.A Using NumPy (we'll do this together)\n\nLet's modify our NumPy implementation to:\n\n1. Output one number per class (logits)\n2. Convert logits to probabilities using softmax\n3. Use cross-entropy loss instead of MSE\n\nWe'll start by using one-hot encoding for the outputs, which will make it easy to implement the cross-entropy loss.","metadata":{}},{"cell_type":"code","source":"# np.eye is the identity matrix; each row is a one-hot vector.\n# this code pulls out the one-hot vectors corresponding to the target values\ny_train_onehot = np.eye(n_classes)[y_train]\ny_valid_onehot = np.eye(n_classes)[y_valid]\n\ny_train_onehot[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T00:00:01.733742Z","iopub.execute_input":"2025-02-28T00:00:01.734288Z","iopub.status.idle":"2025-02-28T00:00:01.740599Z","shell.execute_reply.started":"2025-02-28T00:00:01.734252Z","shell.execute_reply":"2025-02-28T00:00:01.739780Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.]])"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"Now we'll implement softmax and cross-entropy together.","metadata":{}},{"cell_type":"code","source":"def softmax(logits):\n    \"\"\"Convert logits to probabilities.\"\"\"\n    num_samples, num_classes = logits.shape\n\n    max_logit = logits.max(axis=1, keepdims=True)\n\n    exp_logits = np.exp(logits - max_logit)\n    assert exp_logits.shape == (num_samples, num_classes)\n\n    sum_exp_logits = exp_logits.sum(axis=1, keepdims=True)\n    assert sum_exp_logits.shape == (num_samples, 1)\n    return exp_logits / sum_exp_logits\n\ndef cross_entropy_loss(y_true_onehot, probs):\n    \"\"\"Compute cross entropy loss.\"\"\"\n    # Strategy:\n    # - Compute log probabilities\n    # - Multiply by one-hot vectors and sum, to extract the log probabilities of the true classes\n    # - Take the negative to get the loss\n    # - Average the loss over the samples\n\n    log_probs = np.log(probs + 1e-6) # add a small value to avoid log(0)\n    loss_per_sample = -np.sum(y_true_onehot * log_probs, axis=1)\n    assert loss_per_sample.shape == (len(y_true_onehot),)\n    return -np.mean(y_true_onehot * log_probs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:40:50.803995Z","iopub.execute_input":"2025-02-28T01:40:50.804322Z","iopub.status.idle":"2025-02-28T01:40:50.810053Z","shell.execute_reply.started":"2025-02-28T01:40:50.804298Z","shell.execute_reply":"2025-02-28T01:40:50.808982Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# copy and paste the NumPy code block here and we'll edit.\n# Initialize parameters\nnp.random.seed(42)\nn_samples, n_features = X_train.shape\n\ninitial_weights = np.random.standard_normal(size=(2, 3)) # num of features by num of classes\ninitial_bias = np.random.standard_normal(size=3)  # a number for each class\n\ndef linear_forward(X, weights, bias):\n    return X @ weights + bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:40:52.867109Z","iopub.execute_input":"2025-02-28T01:40:52.867443Z","iopub.status.idle":"2025-02-28T01:40:52.872090Z","shell.execute_reply.started":"2025-02-28T01:40:52.867416Z","shell.execute_reply":"2025-02-28T01:40:52.871282Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def loss_given_params(params, X, y_true_onehot):\n    weights, bias = unpack_params(params)\n    y_pred = linear_forward(X, weights, bias)\n    logits = linear_forward(X, weights, bias)\n    probs = softmax(logits)\n    return cross_entropy_loss(y_true_onehot, probs)\n\ninitial_params, unpack_params = flatten([initial_weights, initial_bias])\ninitial_loss = loss_given_params(initial_params, X_train, y_train_onehot)\noptimization_result = scipy.optimize.minimize(loss_given_params, initial_params,\n                                              args=(X_train, y_train_onehot))\nfitted_weights_np, fitted_bias_np = unpack_params(optimization_result.x)\nprint(f\"Initial loss: {initial_loss:.2f}\")\nprint(f\"Final loss: {optimization_result.fun:.2f}\")\nprint(f\"Fitted weights: {fitted_weights_np}\")\nprint(f\"Fitted bias: {fitted_bias_np}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:46:06.080867Z","iopub.execute_input":"2025-02-28T01:46:06.081184Z","iopub.status.idle":"2025-02-28T01:46:06.313182Z","shell.execute_reply.started":"2025-02-28T01:46:06.081162Z","shell.execute_reply":"2025-02-28T01:46:06.312305Z"}},"outputs":[{"name":"stdout","text":"Initial loss: 0.64\nFinal loss: 0.34\nFitted weights: [[ 0.77536606  0.39968742 -0.16891526]\n [ 0.01591506  0.34059967  0.6982245 ]]\nFitted bias: [0.61373843 0.75166814 0.51176642]\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"Now let's compute the accuracy again.","metadata":{}},{"cell_type":"code","source":"y_pred_valid_logits = linear_forward(X_valid, fitted_weights_np, fitted_bias_np)\ny_pred_valid = y_pred_valid_logits.argmax(axis=1)\ny_pred_valid[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:46:18.350420Z","iopub.execute_input":"2025-02-28T01:46:18.350711Z","iopub.status.idle":"2025-02-28T01:46:18.357717Z","shell.execute_reply.started":"2025-02-28T01:46:18.350690Z","shell.execute_reply":"2025-02-28T01:46:18.356811Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"array([0, 1, 2, 2, 0, 2, 0, 1, 2, 2])"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"accuracy = (y_pred_valid == y_valid).mean()\nprint(f\"Validation accuracy for logistic regression: {accuracy:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:46:26.012706Z","iopub.execute_input":"2025-02-28T01:46:26.013034Z","iopub.status.idle":"2025-02-28T01:46:26.017683Z","shell.execute_reply.started":"2025-02-28T01:46:26.013008Z","shell.execute_reply":"2025-02-28T01:46:26.016920Z"}},"outputs":[{"name":"stdout","text":"Validation accuracy for logistic regression: 0.52\n","output_type":"stream"}],"execution_count":58},{"cell_type":"markdown","source":"### 2.B Using PyTorch\n\nWe'll first walk through the PyTorch primitives together, then you'll modify the PyTorch code from Part 1 to use these primitives.","metadata":{}},{"cell_type":"code","source":"y_train_longtensor = torch.tensor(y_train)\ny_valid_longtensor = torch.tensor(y_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:46:33.148134Z","iopub.execute_input":"2025-02-28T01:46:33.148478Z","iopub.status.idle":"2025-02-28T01:46:33.154046Z","shell.execute_reply.started":"2025-02-28T01:46:33.148451Z","shell.execute_reply":"2025-02-28T01:46:33.153255Z"}},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":"#### Setting up the linear layer\n\nFill in the blanks below with the correct number of features.","metadata":{}},{"cell_type":"code","source":"linear_layer = nn.Linear(in_features=2, out_features=3, bias=True)\nlogits = linear_layer(X_train_torch)\nlogits.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:49:21.444881Z","iopub.execute_input":"2025-02-28T01:49:21.445193Z","iopub.status.idle":"2025-02-28T01:49:21.460950Z","shell.execute_reply.started":"2025-02-28T01:49:21.445170Z","shell.execute_reply":"2025-02-28T01:49:21.460256Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"torch.Size([2344, 3])"},"metadata":{}}],"execution_count":62},{"cell_type":"markdown","source":"#### Softmax\n\nPyTorch has builtin functionality for softmax. We can use it like this:","metadata":{}},{"cell_type":"code","source":"probs = logits.softmax(axis=1)\nprobs.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:49:25.948787Z","iopub.execute_input":"2025-02-28T01:49:25.949095Z","iopub.status.idle":"2025-02-28T01:49:25.956477Z","shell.execute_reply.started":"2025-02-28T01:49:25.949071Z","shell.execute_reply":"2025-02-28T01:49:25.955541Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"torch.Size([2344, 3])"},"metadata":{}}],"execution_count":63},{"cell_type":"markdown","source":"Let's check that the probabilities sum to 1 for each row. Think about what axis you should use for the sum. *General sum rule: whatever axis we sum over will be the axis that disappears.*","metadata":{}},{"cell_type":"code","source":"probs_sums = probs.sum(axis=1)\nprobs_sums.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:49:44.626267Z","iopub.execute_input":"2025-02-28T01:49:44.626630Z","iopub.status.idle":"2025-02-28T01:49:44.634227Z","shell.execute_reply.started":"2025-02-28T01:49:44.626599Z","shell.execute_reply":"2025-02-28T01:49:44.633322Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"torch.Size([2344])"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"probs_sums[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:49:49.630098Z","iopub.execute_input":"2025-02-28T01:49:49.630453Z","iopub.status.idle":"2025-02-28T01:49:49.658608Z","shell.execute_reply.started":"2025-02-28T01:49:49.630426Z","shell.execute_reply":"2025-02-28T01:49:49.657755Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000], grad_fn=<SliceBackward0>)"},"metadata":{}}],"execution_count":65},{"cell_type":"markdown","source":"#### Cross-entropy loss\n\nThere are a few different ways we could implement a categorical cross-entropy loss in PyTorch; notice that they give the same output. The `F.cross_entropy` approach is most efficient and numerically stable because **it combines the softmax operation, one-hot encoding, and the cross-entropy loss** into a single step.","metadata":{}},{"cell_type":"code","source":"# Direct approach, analogous to the NumPy code above\ny_train_onehot_torch = torch.tensor(y_train_onehot, dtype=torch.int64)\nlog_probs = probs.log()\n-(y_train_onehot_torch * log_probs).sum(axis=1).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:54:02.375361Z","iopub.execute_input":"2025-02-28T01:54:02.375745Z","iopub.status.idle":"2025-02-28T01:54:02.391763Z","shell.execute_reply.started":"2025-02-28T01:54:02.375719Z","shell.execute_reply":"2025-02-28T01:54:02.390666Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"tensor(1.0945, grad_fn=<NegBackward0>)"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"# cross-entropy loss function in PyTorch uses logits (without softmax) and class indices (without one-hot)\nF.cross_entropy(logits, y_train_longtensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:54:05.530112Z","iopub.execute_input":"2025-02-28T01:54:05.530495Z","iopub.status.idle":"2025-02-28T01:54:05.540507Z","shell.execute_reply.started":"2025-02-28T01:54:05.530465Z","shell.execute_reply":"2025-02-28T01:54:05.539480Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"tensor(1.0945, grad_fn=<NllLossBackward0>)"},"metadata":{}}],"execution_count":67},{"cell_type":"markdown","source":"<details> \n  <summary>Other ways to compute cross-entropy loss</summary>\n\n```python\n # see also F.one_hot if you're curious.\nF.nll_loss(log_probs, y_train_longtensor)\nF.nll_loss(logits.log_softmax(axis=1), y_train_longtensor)\n\n# using the \"object-oriented\" interface\nloss_fn = nn.CrossEntropyLoss()\nloss_fn(logits, y_train_longtensor)\n```\n</details>","metadata":{}},{"cell_type":"markdown","source":"#### Full PyTorch Implementation\n\n**Your turn**: Copy and paste your PyTorch linear regression code here and make it into a logistic regression by modifying it to use softmax and cross-entropy loss.\n","metadata":{}},{"cell_type":"code","source":"# Convert data to PyTorch tensors\nX_train_torch = torch.tensor(X_train, dtype=torch.float32)\nX_valid_torch = torch.tensor(X_valid, dtype=torch.float32)\n\ny_train_torch = torch.tensor(y_train, dtype=torch.float32)\ny_valid_torch = torch.tensor(y_valid, dtype=torch.float32)\n\n# Training loop\nlinear_layer = nn.Linear(in_features=2, out_features=3, bias=True)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\nn_epochs = 100  # epoch--number of times through the training set\nfor epoch in range(n_epochs):\n    # Forward pass\n    logits = linear_layer(X_train_torch)\n    probs = logits.softmax(axis=1)\n    loss = F.cross_entropy(probs, y_train_longtensor)\n    \n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if epoch % 10 == 0:\n        print(f'Epoch {epoch:02d}, Loss: {loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T02:03:54.425947Z","iopub.execute_input":"2025-02-28T02:03:54.426338Z","iopub.status.idle":"2025-02-28T02:03:54.532226Z","shell.execute_reply.started":"2025-02-28T02:03:54.426295Z","shell.execute_reply":"2025-02-28T02:03:54.531328Z"}},"outputs":[{"name":"stdout","text":"Epoch 00, Loss: 1.0524\nEpoch 10, Loss: 1.0524\nEpoch 20, Loss: 1.0524\nEpoch 30, Loss: 1.0524\nEpoch 40, Loss: 1.0524\nEpoch 50, Loss: 1.0524\nEpoch 60, Loss: 1.0524\nEpoch 70, Loss: 1.0524\nEpoch 80, Loss: 1.0524\nEpoch 90, Loss: 1.0524\n","output_type":"stream"}],"execution_count":75},{"cell_type":"markdown","source":"### Looking ahead: a multi-layer network\n\nJust as a preview for where we're going next week:","metadata":{}},{"cell_type":"code","source":"n_hidden = 100\nmodel = nn.Sequential(\n    nn.Linear(in_features=n_features, out_features=n_hidden, bias=True),\n    nn.ReLU(),\n    nn.Linear(in_features=n_hidden, out_features=n_classes, bias=True),\n)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\nn_epochs = 1000\nfor epoch in range(n_epochs):\n    # Forward pass\n    logits = model(X_train_torch)\n    loss = F.cross_entropy(logits, y_train_longtensor)\n    \n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if epoch % 10 == 0:\n        print(f'Epoch {epoch:03d}, Loss: {loss.item():.4f}')\n\n# Evaluate on validation set\nlogits_valid = model(X_valid_torch)\ny_pred_valid = logits_valid.argmax(dim=1)\naccuracy = (y_pred_valid == y_valid_longtensor).float().mean()\nprint(f'Validation accuracy: {accuracy.item():.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T02:03:59.767983Z","iopub.execute_input":"2025-02-28T02:03:59.768356Z","iopub.status.idle":"2025-02-28T02:04:01.397009Z","shell.execute_reply.started":"2025-02-28T02:03:59.768329Z","shell.execute_reply":"2025-02-28T02:04:01.396118Z"}},"outputs":[{"name":"stdout","text":"Epoch 000, Loss: 1.0985\nEpoch 010, Loss: 1.0176\nEpoch 020, Loss: 0.9951\nEpoch 030, Loss: 0.9783\nEpoch 040, Loss: 0.9645\nEpoch 050, Loss: 0.9529\nEpoch 060, Loss: 0.9430\nEpoch 070, Loss: 0.9346\nEpoch 080, Loss: 0.9273\nEpoch 090, Loss: 0.9211\nEpoch 100, Loss: 0.9157\nEpoch 110, Loss: 0.9109\nEpoch 120, Loss: 0.9068\nEpoch 130, Loss: 0.9030\nEpoch 140, Loss: 0.8997\nEpoch 150, Loss: 0.8968\nEpoch 160, Loss: 0.8941\nEpoch 170, Loss: 0.8916\nEpoch 180, Loss: 0.8894\nEpoch 190, Loss: 0.8874\nEpoch 200, Loss: 0.8856\nEpoch 210, Loss: 0.8840\nEpoch 220, Loss: 0.8825\nEpoch 230, Loss: 0.8811\nEpoch 240, Loss: 0.8798\nEpoch 250, Loss: 0.8785\nEpoch 260, Loss: 0.8774\nEpoch 270, Loss: 0.8763\nEpoch 280, Loss: 0.8753\nEpoch 290, Loss: 0.8743\nEpoch 300, Loss: 0.8734\nEpoch 310, Loss: 0.8725\nEpoch 320, Loss: 0.8716\nEpoch 330, Loss: 0.8707\nEpoch 340, Loss: 0.8698\nEpoch 350, Loss: 0.8690\nEpoch 360, Loss: 0.8681\nEpoch 370, Loss: 0.8673\nEpoch 380, Loss: 0.8666\nEpoch 390, Loss: 0.8658\nEpoch 400, Loss: 0.8651\nEpoch 410, Loss: 0.8643\nEpoch 420, Loss: 0.8636\nEpoch 430, Loss: 0.8629\nEpoch 440, Loss: 0.8622\nEpoch 450, Loss: 0.8615\nEpoch 460, Loss: 0.8608\nEpoch 470, Loss: 0.8601\nEpoch 480, Loss: 0.8595\nEpoch 490, Loss: 0.8588\nEpoch 500, Loss: 0.8581\nEpoch 510, Loss: 0.8575\nEpoch 520, Loss: 0.8569\nEpoch 530, Loss: 0.8563\nEpoch 540, Loss: 0.8557\nEpoch 550, Loss: 0.8551\nEpoch 560, Loss: 0.8545\nEpoch 570, Loss: 0.8539\nEpoch 580, Loss: 0.8533\nEpoch 590, Loss: 0.8527\nEpoch 600, Loss: 0.8522\nEpoch 610, Loss: 0.8516\nEpoch 620, Loss: 0.8511\nEpoch 630, Loss: 0.8505\nEpoch 640, Loss: 0.8499\nEpoch 650, Loss: 0.8494\nEpoch 660, Loss: 0.8488\nEpoch 670, Loss: 0.8483\nEpoch 680, Loss: 0.8478\nEpoch 690, Loss: 0.8473\nEpoch 700, Loss: 0.8468\nEpoch 710, Loss: 0.8463\nEpoch 720, Loss: 0.8458\nEpoch 730, Loss: 0.8453\nEpoch 740, Loss: 0.8448\nEpoch 750, Loss: 0.8443\nEpoch 760, Loss: 0.8439\nEpoch 770, Loss: 0.8434\nEpoch 780, Loss: 0.8430\nEpoch 790, Loss: 0.8425\nEpoch 800, Loss: 0.8421\nEpoch 810, Loss: 0.8417\nEpoch 820, Loss: 0.8412\nEpoch 830, Loss: 0.8408\nEpoch 840, Loss: 0.8404\nEpoch 850, Loss: 0.8400\nEpoch 860, Loss: 0.8396\nEpoch 870, Loss: 0.8392\nEpoch 880, Loss: 0.8387\nEpoch 890, Loss: 0.8383\nEpoch 900, Loss: 0.8379\nEpoch 910, Loss: 0.8375\nEpoch 920, Loss: 0.8371\nEpoch 930, Loss: 0.8368\nEpoch 940, Loss: 0.8364\nEpoch 950, Loss: 0.8360\nEpoch 960, Loss: 0.8356\nEpoch 970, Loss: 0.8352\nEpoch 980, Loss: 0.8349\nEpoch 990, Loss: 0.8345\nValidation accuracy: 0.61\n","output_type":"stream"}],"execution_count":76},{"cell_type":"markdown","source":"## Analysis\n\nCome back to these questions after you've finished the lab.\n\n1. Compare the NumPy and PyTorch implementations. What are the main differences you notice? What are the benefits of PyTorch?\n\n2. We came up with 4 weight matrices in the main part of this exercise (linear regression NumPy and PyTorch, logistic regression NumPy and PyTorch). Which of these matrices had the same shapes? Which ones had the same values? Why?\n\n3. Which of these two models (linear regression vs logistic regression) does better on this task? (Which number would we use to compare them: loss, accuracy, or something else? Why?)\n\n4. Suppose a job interviewer asks you to describe the similarities and differences between linear regression and logistic regression. What would you say? (Hint: discuss how each model makes a prediction, what kinds of patterns in the data they can use, how you measure training progress, etc.","metadata":{}},{"cell_type":"markdown","source":"1. The main difference between `Numpy` and `PyTorch` is that PyTorch has a bunch of functions that we don't have to build from scratch (e.g. `cross-entropy` and `softmax`)--it's more convenient. Another difference is with how we're representing data. In Numpy we're using arrays whereas in PyTorch we utilize `tensors`.\n2. `blank`\n3. The `Logistic Regression Model` performs the best. The metric we use to make this conclusion is the accuracy. We can also use loss to tell which worked better because even when both models started from the same point, the loss was a lot less in `Logistic Reg Model` than the `Linear Model`.\n4. Both methods utilize the same `forward pass` technique to a **point**--specifically the sum of the product (of the training data, and weight) with the bias. However, where they differ is how they calculate loss. With linear regression we are dealing with numerical (continuous) samples; hence why we metrics such as MSE or MAE. In contrast, the logistic regression works with **discrete** classes. Here we're focused on metrics such as **accuracy**.","metadata":{}}]}