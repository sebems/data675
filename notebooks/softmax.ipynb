{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Softmax, part 1\n\nTask: practice using the `softmax` function.\n\n**Why**: The softmax is a building block that is used throughout machine learning, statistics, data modeling, and even statistical physics. This activity is designed to get comfortable with how it works at a high and low level.\n\n**Note**: Although \"softmax\" is the conventional name in machine learning, you may also see it called \"soft *arg* max\". The [Wikipedia article](https://en.wikipedia.org/w/index.php?title=Softmax_function&oldid=1065998663) has a good explanation.","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import tensor\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T06:34:24.145998Z","iopub.execute_input":"2025-02-19T06:34:24.146225Z","iopub.status.idle":"2025-02-19T06:34:27.544192Z","shell.execute_reply.started":"2025-02-19T06:34:24.146205Z","shell.execute_reply":"2025-02-19T06:34:27.543317Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Task","metadata":{}},{"cell_type":"markdown","source":"The following function defines `softmax` by using PyTorch built-in functionality.","metadata":{}},{"cell_type":"code","source":"def softmax_torch(x):\n    '''Compute the softmax along the last axis, using PyTorch'''\n    # axis=-1 means the last axis\n    # This won't matter in this exercise, but it will matter when we get to batches of data.\n    return torch.softmax(x, axis=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T06:35:05.236280Z","iopub.execute_input":"2025-02-19T06:35:05.236619Z","iopub.status.idle":"2025-02-19T06:35:05.240819Z","shell.execute_reply.started":"2025-02-19T06:35:05.236592Z","shell.execute_reply":"2025-02-19T06:35:05.239962Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Let's try the PyTorch softmax on an example tensor.","metadata":{},"attachments":{}},{"cell_type":"code","source":"x = tensor([1., 2., 3.])\nsoftmax_torch(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T06:40:49.592326Z","iopub.execute_input":"2025-02-19T06:40:49.592627Z","iopub.status.idle":"2025-02-19T06:40:49.598703Z","shell.execute_reply.started":"2025-02-19T06:40:49.592606Z","shell.execute_reply":"2025-02-19T06:40:49.597990Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"tensor([0.0900, 0.2447, 0.6652])"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"2. Fill in the following function to implement softmax yourself:","metadata":{}},{"cell_type":"code","source":"def softmax(xx):\n    # Exponentiate x so all numbers are positive.\n    expos = xx.exp()\n    assert expos.min() >= 0\n    # Normalize (divide by the sum).\n    return expos / sum(expos)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T06:42:26.681162Z","iopub.execute_input":"2025-02-19T06:42:26.681475Z","iopub.status.idle":"2025-02-19T06:42:26.685464Z","shell.execute_reply.started":"2025-02-19T06:42:26.681445Z","shell.execute_reply":"2025-02-19T06:42:26.684580Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"3. Evaluate `softmax(x)` and verify (visually) that it is close to the `softmax_torch(x)` you evaluated above.","metadata":{},"attachments":{}},{"cell_type":"code","source":"softmax(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T06:42:30.553642Z","iopub.execute_input":"2025-02-19T06:42:30.553960Z","iopub.status.idle":"2025-02-19T06:42:30.571744Z","shell.execute_reply.started":"2025-02-19T06:42:30.553927Z","shell.execute_reply":"2025-02-19T06:42:30.570881Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tensor([0.0900, 0.2447, 0.6652])"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"4. Evaluate each of the following expressions (to make sure you understand what the input is), then evaluate `softmax_torch(__)` for each of the following expressions. Observe how each output relates to `softmax_torch(x)`. (Is it the same? Is it different? Why?)\n\n- `x + 1`\n- `x - 100`\n- `x - x.max()`\n- `x * 0.5`\n- `x * 3.0`","metadata":{},"attachments":{}},{"cell_type":"code","source":"x + 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T06:52:49.053771Z","iopub.execute_input":"2025-02-19T06:52:49.054095Z","iopub.status.idle":"2025-02-19T06:52:49.060020Z","shell.execute_reply.started":"2025-02-19T06:52:49.054070Z","shell.execute_reply":"2025-02-19T06:52:49.059254Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([2., 3., 4.])"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"softmax_torch(x * 3.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T06:53:44.746274Z","iopub.execute_input":"2025-02-19T06:53:44.746583Z","iopub.status.idle":"2025-02-19T06:53:44.753116Z","shell.execute_reply.started":"2025-02-19T06:53:44.746561Z","shell.execute_reply":"2025-02-19T06:53:44.752360Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"tensor([0.0024, 0.0473, 0.9503])"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"*Same or different as `softmax_torch(x)`? Why?*","metadata":{}},{"cell_type":"markdown","source":"(repeat for the other expressions)","metadata":{}},{"cell_type":"markdown","source":">For the first 3 operations, the `softmax_torch` operation was the same. The last two were different because we're adding a scale to each of the exponents. The difference is due to the behavior of exponents. Adding to an exponent in a softmax equation will make the exponent of the constant 'cancel out'. However, the multiplication (or division) will cause change.","metadata":{}},{"cell_type":"markdown","source":"## Analysis","metadata":{}},{"cell_type":"markdown","source":"1. A valid probability distribution has no negative numbers and sums to 1. Is `softmax(x)` a valid probability distribution? Why or why not?","metadata":{}},{"cell_type":"markdown","source":">The `softmax` method is a valid distribution. The formula involves us dividing from a 'whole' (i.e. `sum(exp(x))`). It's only reasonable that sum of all the `probs` would equate to **1**.","metadata":{}},{"cell_type":"markdown","source":"Consider the following situation:","metadata":{}},{"cell_type":"code","source":"y2 = tensor([1., 0.,])\ny3 = y2 - 1\ny3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:03:44.443459Z","iopub.execute_input":"2025-02-19T07:03:44.443839Z","iopub.status.idle":"2025-02-19T07:03:44.451292Z","shell.execute_reply.started":"2025-02-19T07:03:44.443809Z","shell.execute_reply":"2025-02-19T07:03:44.450511Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"tensor([ 0., -1.])"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"y4 = y2 * 2\ny4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:03:59.882515Z","iopub.execute_input":"2025-02-19T07:03:59.882848Z","iopub.status.idle":"2025-02-19T07:03:59.889462Z","shell.execute_reply.started":"2025-02-19T07:03:59.882824Z","shell.execute_reply":"2025-02-19T07:03:59.888662Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([2., 0.])"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"1. Are `softmax(y2)` and `softmax(y3)` the same or different? How could you tell without having to evaluate them?\n","metadata":{}},{"cell_type":"markdown","source":">Both functions will return the same result. We aren't scaling (i.e. multiplying).","metadata":{}},{"cell_type":"markdown","source":"2. Are `softmax(y2)` and `softmax(y4)` the same or different? How could you tell without having to evaluate them?\n","metadata":{}},{"cell_type":"markdown","source":">Both functions will return different results, due to multiplication of a `scale`.","metadata":{}},{"cell_type":"markdown","source":"## Optional Extension: Numerical Issues","metadata":{}},{"cell_type":"markdown","source":"### Task for Numerical Issues","metadata":{},"attachments":{}},{"cell_type":"markdown","source":"5. *Numerical issues*. Assign `x2 = 50 * x`. Try `softmax(x2)` and observe that the result includes the dreaded `nan` -- \"not a number\". Something went wrong. **Evaluate the first mathematical operation in `softmax`** for this particularly problematic input. You should see another kind of abnormal value.","metadata":{}},{"cell_type":"code","source":"x2 = 50 * x\nsoftmax(x2)","metadata":{},"outputs":[{"data":{"text/plain":["tensor([0., nan, nan])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"execution_count":null},{"cell_type":"code","source":"# your code here (the first mathematical operation in `softmax`)","metadata":{},"outputs":[{"data":{"text/plain":["tensor([5.1847e+21,        inf,        inf])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"execution_count":null},{"cell_type":"markdown","source":"6. *Fixing numerical issues*. Now try `softmax(x2 - 150.0)`. Observe that you now get valid numbers. Also observe how the constant we subtracted relates to the value of `x2`.","metadata":{}},{"cell_type":"code","source":"# your code here","metadata":{},"outputs":[{"data":{"text/plain":["tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"execution_count":null},{"cell_type":"markdown","source":"7. Copy your `softmax` implementation to a new function, `softmax_stable`, and change it so that it subtracts `xx.max()` from `xx` before exponentiating. (Don't use any in-place operations; just use `xx - xx.max()`) Verify that `softmax_stable(x2)` now works, and obtains the same result as `softmax_torch(x2)`","metadata":{},"attachments":{}},{"cell_type":"code","source":"# your code here","metadata":{},"outputs":[{"data":{"text/plain":["tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"execution_count":null},{"cell_type":"code","source":"softmax_torch(x2)","metadata":{},"outputs":[{"data":{"text/plain":["tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"execution_count":null},{"cell_type":"markdown","source":"### Analysis of Numerical Issues","metadata":{},"attachments":{}},{"cell_type":"markdown","source":"3. Explain why `softmax(x2)` failed.","metadata":{}},{"cell_type":"markdown","source":"*your answer here*","metadata":{}},{"cell_type":"markdown","source":"4. Use your observations in \\#1-2 above to explain why `softmax_stable` still gives the correct answer for `x` even though we changed the input.","metadata":{},"attachments":{}},{"cell_type":"markdown","source":"*your answer here*","metadata":{}},{"cell_type":"markdown","source":"5. Explain why `softmax_stable` doesn't give us infinity or Not A Number anymore for `x2`.","metadata":{},"attachments":{}},{"cell_type":"markdown","source":"*your answer here*","metadata":{}},{"cell_type":"markdown","source":"## Extension *optional*\n\nTry to prove your observation in Analysis \\#1 by symbolically simplifying the expression `softmax(logits + c)` and seeing if you can get `softmax(logits)`. Remember that `softmax(x) = exp(x) / exp(x).sum()` and `exp(a + b) = exp(a)exp(b)`.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}